{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc6a6bd",
   "metadata": {},
   "source": [
    "# Sklearn per applicazioni di NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfce9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd266f69",
   "metadata": {},
   "source": [
    "##Â Natural Language Processing\n",
    "\n",
    "SpaCy: [https://spacy.io/](https://spacy.io/)\n",
    "\n",
    "NLTK: [https://www.nltk.org/](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bf711b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71db8d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8af76770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0faa96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6e33a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(data_home='/Users/flint/Data/sklearn/', remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e10a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = lambda x: [w for w in word_tokenize(x.lower()) if w not in string.punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08cef6e",
   "metadata": {},
   "source": [
    "## Term Frequency (TF)\n",
    "\n",
    "$$\n",
    "tf(w, d) = count(w) \\in d\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe348cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cdc76dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(token_pattern=None,\n",
       "                tokenizer=<function <lambda> at 0x7f9c5c70d310>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = CountVectorizer(tokenizer=tokenizer, token_pattern=None)\n",
    "cnt.fit(dataset.data[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "948f38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = cnt.transform(dataset.data[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d3475b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad09d59b",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency (IDF)\n",
    "\n",
    "$$\n",
    "idf(w) = \\log \\frac{N}{\\mid \\{d : w \\in d \\} \\mid}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a0c02ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfm = TfidfVectorizer(tokenizer=tokenizer, token_pattern=None)\n",
    "tfidfm.fit(dataset.data[:2000])\n",
    "tfidf = tfidfm.transform(dataset.data[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4438fd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10892730473248623"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[0, tfidfm.vocabulary_['history']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02123ef1",
   "metadata": {},
   "source": [
    "### Measuring relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "979f88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8540b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance(document_id, model, matrix):\n",
    "    tokens = tokenizer(dataset.data[document_id])\n",
    "    word_score = {}\n",
    "    for token in set(tokens):\n",
    "        score = matrix[document_id, model.vocabulary_[token]]\n",
    "        word_score[token] = score\n",
    "    return sorted(word_score.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b7208",
   "metadata": {},
   "source": [
    "## Data Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "86ce674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(\n",
    "    data_home='/Users/flint/Data/sklearn/', \n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    subset='train'\n",
    ")\n",
    "test = fetch_20newsgroups(\n",
    "    data_home='/Users/flint/Data/sklearn/', \n",
    "    remove=('headers', 'footers', 'quotes'),\n",
    "    subset='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab759981",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = defaultdict(lambda: 0)\n",
    "for k in train.target:\n",
    "    class_name = train.target_names[k]\n",
    "    counter[class_name] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e9c92bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calcolo i parametri di TfIdf e trasformo il train set\n",
      "Applico tfidf sul test set\n"
     ]
    }
   ],
   "source": [
    "m = TfidfVectorizer(tokenizer=tokenizer, token_pattern=None, min_df=50)\n",
    "print('Calcolo i parametri di TfIdf e trasformo il train set')\n",
    "tfidf_train = m.fit_transform(train.data)\n",
    "print('Applico tfidf sul test set')\n",
    "tfidf_test = m.transform(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "14520f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 2890)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8a325a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f7d597c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GaussianNB()\n",
    "classifier.fit(tfidf_train.toarray(), train.target)\n",
    "y_prediction = classifier.predict(tfidf_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bab28bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  5,  2, ..., 15,  6, 15])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "acafea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "997a03a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.20      0.21       319\n",
      "           1       0.29      0.21      0.24       389\n",
      "           2       0.22      0.32      0.26       394\n",
      "           3       0.31      0.33      0.32       392\n",
      "           4       0.23      0.29      0.26       385\n",
      "           5       0.46      0.33      0.38       395\n",
      "           6       0.48      0.43      0.45       390\n",
      "           7       0.41      0.26      0.32       396\n",
      "           8       0.44      0.38      0.41       398\n",
      "           9       0.24      0.47      0.32       397\n",
      "          10       0.62      0.55      0.58       399\n",
      "          11       0.49      0.57      0.53       396\n",
      "          12       0.28      0.22      0.24       393\n",
      "          13       0.48      0.30      0.37       396\n",
      "          14       0.43      0.41      0.42       394\n",
      "          15       0.37      0.39      0.38       398\n",
      "          16       0.31      0.36      0.33       364\n",
      "          17       0.53      0.55      0.54       376\n",
      "          18       0.24      0.18      0.21       310\n",
      "          19       0.16      0.17      0.16       251\n",
      "\n",
      "    accuracy                           0.35      7532\n",
      "   macro avg       0.36      0.35      0.35      7532\n",
      "weighted avg       0.37      0.35      0.35      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test.target, y_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e18156",
   "metadata": {},
   "source": [
    "## Co-occorrenze\n",
    "- Indice di occorrenze delle parole\n",
    "-- Indice inverso : key: word value: lista dei documenti in cui compare word\n",
    "- Misura di rivlevanza delle co-occorrenze che osserviamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "18f8e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2000\n",
    "documents = train.data[:N]\n",
    "inverted_index = defaultdict(lambda: set())\n",
    "for i, doc in enumerate(documents):\n",
    "    tokens = tokenizer(doc)\n",
    "    for token in tokens:\n",
    "        inverted_index[token].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1bc2b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word, docs in inverted_index.items() if len(docs) > 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12778d7e",
   "metadata": {},
   "source": [
    "$$\n",
    "MI(a, b) = \\log \\frac{P(a,b)}{P(a)P(b)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1a69d535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(word):\n",
    "    c = len(inverted_index[word])\n",
    "    return c / N\n",
    "\n",
    "def pp(w1, w2):\n",
    "    c = len(inverted_index[w1].intersection(inverted_index[w2]))\n",
    "    return c / N\n",
    "\n",
    "def mi(w1, w2):\n",
    "    if p(w1)*p(w2) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.log(pp(w1, w2) / (p(w1)*p(w2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cb15157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cn/1cnswmps6xsbv3x70vr8z3v80000gn/T/ipykernel_2551/3590744656.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(pp(w1, w2) / (p(w1)*p(w2)))\n"
     ]
    }
   ],
   "source": [
    "collector = []\n",
    "for i, word1 in enumerate(words):\n",
    "    for word2 in words[i+1:]:\n",
    "        collector.append({\n",
    "            'word1': word1,\n",
    "            'word2': word2,\n",
    "            'sim': mi(word1, word2)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "448e5980",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.DataFrame(collector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6d245491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>was</td>\n",
       "      <td>0.177190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i</td>\n",
       "      <td>wondering</td>\n",
       "      <td>0.358820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i</td>\n",
       "      <td>if</td>\n",
       "      <td>0.201099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i</td>\n",
       "      <td>anyone</td>\n",
       "      <td>0.217355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i</td>\n",
       "      <td>out</td>\n",
       "      <td>0.224648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word1      word2       sim\n",
       "0     i        was  0.177190\n",
       "1     i  wondering  0.358820\n",
       "2     i         if  0.201099\n",
       "3     i     anyone  0.217355\n",
       "4     i        out  0.224648"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1389aeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831405, 3)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cdd62b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>wondering</td>\n",
       "      <td>appreciated</td>\n",
       "      <td>2.055725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3456</th>\n",
       "      <td>wondering</td>\n",
       "      <td>baseball</td>\n",
       "      <td>2.055725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14150</th>\n",
       "      <td>car</td>\n",
       "      <td>model</td>\n",
       "      <td>2.112883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14152</th>\n",
       "      <td>car</td>\n",
       "      <td>engine</td>\n",
       "      <td>2.371578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14672</th>\n",
       "      <td>car</td>\n",
       "      <td>cars</td>\n",
       "      <td>2.664315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word1        word2       sim\n",
       "2874   wondering  appreciated  2.055725\n",
       "3456   wondering     baseball  2.055725\n",
       "14150        car        model  2.112883\n",
       "14152        car       engine  2.371578\n",
       "14672        car         cars  2.664315"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F = D[D.sim > 2.0]\n",
    "F.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9988e04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44232, 3)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "011e565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dc4054d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for i, row in D[D.sim > 2.0].iterrows():\n",
    "    G.add_edge(row.word1, row.word2, sim=row.sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9681d29d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1387aefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44232"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8dd6e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2dd98fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'neither', 'night', 'parts', 'assuming', 'changed', 'quickly', 'differences', 'lives', 'months', 'treatment', 'strong', 'kind', 'million', 'numbers', 'giving', 'voice', 'books', 'jewish', 'realize', 'mr.', 'exactly', 'covered', 'cause', 'according', 'second', 'hands', 'administration', 'takes', 'political', 'protect', 'created', 'tell', 'experience', 'recall', 'fire', 'press', 'street', 'comment', 'average', 'christianity', 'cover', 'earth', 'jesus', 'crime', 'matter', 'thinking', 'killed', 'glad', 'women', 'allowed', 'others', 'btw', 'surprised', 'came', 'release', 'king', 'understand', 'presented', 'genocide', 'once', 'behind', 'save', 'talk', 'community', 'law', 'ground', 'lots', 'case', 'sold', 'calling', 'missing', 'whatever', 'difficult', 'living', 'april', 'friends', 'heaven', 'old', 'usually', 'mostly', 'again', 'gave', 'stay', 'eventually', 'letter', 'show', 'policy', 'fair', 'federal', 'clearly', 'possibly', 'across', 'hope', 'authority', 'today', 'responsibility', 'inside', 'civil', 'building', 'soon', 'away', 'among', 'coming', 'job', 'spent', 'looked', 'set', 'expect', 'refer', 'cut', 'wants', 'pick', 'saw', 'through', 'return', 'move', 'claim', 'mother', 'religious', 'cheap', 'month', 'prevent', 'past', 'brought', 'land', 'greater', 'unfortunately', 'damn', 'immediately', 'lost', 'push', 'events', 'prove', 'capable', 'prefer', 'president', 'hours', 'self', 'knew', 'hit', 'reading', 'decision', 'minutes', 'totally', 'men', 'head', 'laws', 'normally', 'expected', 'necessarily', 'beliefs', 'follow', 'happen', 'considered', 'bible', 'large', 'dead', 'upon', 'certainly', 'apparently', 'cases', 'life', 'single', 'turned', 'mean', 'citizens', 'truth', 'area', 'school', 'banks', 'result', 'imagine', 'discuss', 'obviously', 'bus', 'food', 'war', 'share', 'knows', 'word', 'murder', 'comes', 'likely', '4.', 'international', 'poor', 'red', 'down', 'answer', 'besides', 'near', 'ability', 'forget', 'kill', 'shot', 'pointed', 'side', 'step', 'year', 'effect', 'local', 'simply', 'maybe', 'choice', 'tried', 'suggest', 'lack', 'waste', 'man', 'hold', '1.', 'weapons', 'road', 'either', 'course', 'mind', 'huge', 'absolutely', 'worked', 'home', 'sometimes', 'happy', 'rest', 'bad', 'arguments', 'anyway', 'death', 'meaning', 'assume', 'best', 'methods', 'short', 'ii', 'story', 'entire', 'follows', 'claimed', 'signal', 'children', 'myself', 'four', 'happens', 'her', 'jews', 'attempt', 'call', 'level', 'established', 'thought', 'couple', 'reasons', 'national', 'period', 'hey', 'god', 'worry', 'always', 'create', 'place', 'ever', 'days', 'properly', 'truly', 'care', 'process', 'accept', 'hate', 'continue', 'somewhere', 'effective', 'majority', 'claims', 'obvious', 'during', 'logic', 'particularly', 'involved', 'years', 'wanted', 'safe', 'easily', 'five', 'consider', 'based', 'whether', 'faith', 'happened', 'attack', 'serious', 'individual', 'hand', 'members', 'society', '5.', 'speak', 'meant', 'merely', 'helps', 'agree', 'easy', 'somehow', 'father', 'longer', 'condition', 'means', 'court', 'big', 'wrong', 'station', 'america', 'force', 'government', 'present', 'armenians', 'small', 'himself', 'half', 'statement', 'answers', 'certain', 'charge', 'situation', 'gets', 'usual', 'issue', 'killing', 'mention', 'admit', 'close', 'gotten', 'actions', 'middle', 'become', 'common', 'police', 'three', 'accepted', 'united', 'indeed', 'sign', 'him', 'suspect', 'noticed', 'family', 'ago', 'along', 'light', 'effort', 'proper', 'mentioned', 'europe', 'wrote', 'military', 'needed', 'choose', 'bought', 'reason', 'argument', 'reference', 'considering', 'canada', 'christian', 'population', 'ok', 'alone', 'opinions', 'reported', '30', 'everyone', 'given', 'bring', 'talking', 'manner', 'come', 'israel', 'term', 'taking', 'avoid', 'seeing', 'basically', 'day', 'worse', 'third', 'went', 'itself', 'bet', 'direction', 'event', 'rule', 'therefore', 'facts', 'power', 'business', 'hundreds', 'student', 'rights', 'none', 'saying', 'determine', 'apply', 'especially', 'nobody', 'us', 'nothing', 'wait', 'against', 'several', 'stupid', 'human', 'hell', 'church', 'record', 'carry', 'someone', 'between', 'responsible', 'took', 'read', 'position', 'leads', 'later', 'water', 'quality', 'public', 'six', 'caused', 'door', 'chance', 'wife', 'ones', 'together', 'christ', 'weeks', 'media', 'making', 'guys', 'held', 'asked', 'week', 'similar', 'yourself', '....', 'instance', 'quote', 'army', 'she', 'replaced', 'keep', 'front', 'stated', 'heat', 'u.s.', 'told', 'late', 'western', 'pull', 'love', 'control', 'die', 'everything', 'remember', 'fully', 'gun', 'taken', 'calls', 'doing', 'worth', 'house', 'instead', '2.', 'true', 'belief', 'act', 'perfect', 'finally', 'i.e', 'guns', 'willing', 'trying', 'whose', 'open', 'drugs', 'face', 'decided', 'decide', 'able', 'important', 'leave', 'sort', 'guy', 'action', 'turn', 'deal', 'exist', 'quite', 'gone', 'history', 'started', 'young', '1990', 'room', 'personally', 'supply', 'somebody', 'supposed', 'city', 'book', 'shall', 'nor', 'states', 'pretty', 'news', 'member', 'themselves', 'understanding', 'published', 'evidence', 'defense', 'religion', 'stop', 'allow', 'age', 'doubt', 'explain', 'says', 'tv', 'child', 'live', 'reports', 'caught', 'specifically', 'another', 'completely', 'trouble', 'played', 'nature', 'judge', 'ways', 'free', 'wonder', 'otherwise', 'nearly', 'times', 'risk', 'every', 'country', 'american', 'whole', 'hear', 'dangerous', 'places', 'fine', 'moral', 'full', 'major', 'perhaps', 'yes', '3.', 'opinion', 'believe', 'watch', 'seem', 'ahead', 'clear', 'former', 'normal', 'okay', 'private', 'fall', 'outside', 'christians', 'pay', 'easier', 'sunday', 'sense', 'safety', 'world', 'person', 'left', 'regardless', 'must', 'context', 'thus'})\n",
      "frozenset({'ie', 'n', 'names', 'fairly', 'known', 'ad', 'uses', 'newsgroup', '300', 'needs', 'newsgroups', '0', 'simple', 'graphics', 'range', 'depends', 'currently', 'within', '20', 'regular', '14', 'e', 'technology', 'r', 'cost', '10', 'language', 'code', 'driver', 'details', 'research', 'greatly', 'installed', 'both', 'posts', 'write', '24', '13', 'gas', 'search', 'points', 'display', 'issues', '21', 'results', 'post', 'slow', 'card', 'p', 'written', 'window', 'additional', 'versions', 'knowledge', 'current', 'keys', 'relevant', 'gives', 'summer', 'application', 'works', 'parallel', 'ideas', 'provide', 'price', 'sufficient', 'excellent', '22', '3.1', 'send', 'state', '23', 'fix', '1992', 'including', 'phone', 'article', 'runs', 'version', 'output', 'approach', '8', 'mark', 'break', 'value', 'serial', 'running', 'mode', 'digital', 'software', 'source', 'general', '1', 'useful', 'apple', 'hockey', 'supports', '2', 'chips', 'build', '40', 'organization', 'handle', 'season', 'advantage', 'received', 'main', 'operation', 'copies', 'interest', 'win', '70', 'board', 'command', 's', 'multiple', 'terms', 'apr', 'via', 'manager', 'washington', 'color', 'san', 'john', 'video', 'steve', 'na', 'warning', 'start', 'appreciated', 'o', 'development', '11', 'extra', 'rules', 'chip', 'college', 'd', 'recently', 'discussion', 'internet', 'costs', 'view', 'rate', 'paul', 'above', 'department', 'real', 'university', 'image', 'california', 'internal', 'basic', 'bob', 'limited', 'pass', 'special', 'response', 'future', 'memory', 'difference', 'j.', 'ibm', 'include', 'folks', 'agency', 'posting', 'due', 'mail', 'wondering', 'date', 'tom', 'final', 'groups', 'comparison', 'related', 'correct', 'built', 'systems', 'name', 'required', 'early', 'low', 'model', 'speed', 'access', 'particular', 'recent', 'product', '5', 'ma', 'hardware', 'list', 'text', '26', 'sent', 'check', 'per', 'faster', '60', 'mass', 'david', 'monitor', 'computer', 'plan', 'dave', 'site', 'questions', 'description', 'luck', 'total', 'sources', 'note', 'experiences', 'attention', 'summary', '25', 'white', 'users', 'direct', 'center', 'writing', 'following', 'jim', 'program', '35', 'controller', '50', 'available', 'below', '9', 'advance', 'body', 'texas', 'directly', 'lines', 'radio', '12', 'lower', 'appears', 'machines', 'different', 'applications', 'begin', 'mb', 'game', 'overall', 'standard', 'modern', 'money', 'machine', 'line', 'network', 'specific', 'player', 'complex', 'designed', 'test', 'notice', 'form', 'electronic', 'switch', 'require', 'interesting', 'request', 'paper', 'oil', 'images', 'security', 'box', 'manual', 'dos', '4', '15', 'comments', 'communications', 'technical', 'sun', 'drives', 'described', 'trade', 'ram', '100', 'section', 'original', 'engine', 'shows', '32', 'tape', 'division', 'mac', 'info', 'fax', 'package', 'wo', 'drive', 'server', 'study', 'hard', 'base', 'design', 'expensive', 'performance', 'values', '2nd', 'input', 'articles', 'straight', 'windows', 'bill', 'file', '6', 'screen', 'errors', 'device', 'clinton', 'amount', 'scsi', 'programming', 'information', 'c', 'typical', 'space', 'offer', 'convert', 'project', 'baseball', 'asking', 'error', 'style', '80', 'f', 'fast', 'objective', 'high', '17', 'hello', 'faq', 'basis', 't', 'buy', 'requires', 'aware', 'science', 'games', 'higher', 'service', 'increase', 'company', 'reasonable', 'series', 'x', 'intended', 'independent', 'drivers', 'references', 'size', 'cards', 'user', '45', 'office', 'connection', 'programs', 'al', 'posted', 'support', 'tools', 'w', 'selling', 'pc', 'sale', 'microsoft', 'produce', 'appropriate', 'latest', 'market', '1993', 'secret', 'clipper', 'port', '19', 'addition', 'email', 'play', 'format', 'air', 'm', 'encryption', 'files', 'and/or', 'sell', 'function', 'west', 'solution', 'changes', '18', 'disk', 'players', 'equipment', 'key', 'includes', 'ftp', '3', 'various', 'advice', 'modem', 'complete', 'shipping', 'supported', 'add', 'address', 'report', 'team', 'each', 'data', 'theory', 'concerned', 'message', 'hot', 'ask', 'personal', 'contact', 'included', 'plus', 'class', 'copy', '75', 'b', 'part', 'previous', '7', '16', 'north', 'league'})\n",
      "frozenset({'next', 'done', 'respond', 'significant', 'wish', 'appear', 'generally', 'often', 'change', 'until', 'legal', 'lead', 'sound', 'almost', 'necessary', 'example', 'black', 'beyond', 'subject', 'words', 'end', 'feel', 'number', 'services', 'except', \"don't\", 'figure', 'account', 'purpose', 'heard', 'although', 'makes', 'regards', 'further', 'official', 'excuse', 'type', 'looks', 'top', 'james', 'called', 'fun', 'sorry', 'appreciate', 'unless', 'goes', 'nice', 'found', 'order', 'earlier', 'less', 'entirely', 'already', 'group'})\n",
      "frozenset({'yeah', 'net', 'mike', 'oh', 'seriously', 'thank', 'working'})\n",
      "frozenset({'buying', 'miles', 'car', 'bike', 'cars'})\n",
      "frozenset({'e-mail', 'reply'})\n",
      "frozenset({'mine', 'friend'})\n",
      "frozenset({'deleted', 'stuff'})\n"
     ]
    }
   ],
   "source": [
    "for community in greedy_modularity_communities(G, weight='sim'):\n",
    "    print(community)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd2161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
